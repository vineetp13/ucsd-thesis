
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%"build theory, think expansively, and tackle broader conceptual issues, not (primarily) to reiterate the literal outcomes of your work.”-srk
Drawing from empirical results, related work, and the lessons from designing these systems, this chapter provides future steps along three themes: 1) building future system contributions, 2) identifying open questions for theoretical contributions, and 3) thinking through broader implications for computation in society.
%for designing new systems and for contributing novel theory. 

%%%%%%%%%%%%%%%%%%%%%%
\section{Systems/domains} 
% "where else could your techniques and insights apply? Think big."-srk
% fig: a set that shows many approaches for generating hypotheses and evaluating theories
%-- and says these systems are all waiting to be built

Domain experts make creative contributions like writing articles, curating museums, leading teams, and more. How might we enable a federation of novices to perform these activities? This dissertation demonstrates that procedural guidance works well for scientific experimentation. How might we develop techniques to conceptually divide other scientific activities into small-but-significant tasks that people can perform? Useful techniques for just-in-time expertise include learning, community inputs, automation, and expert help. Futhermore, which other genre of work and levels of contribution could this extend to? As in science, the number of experts in many domains is relatively small and their training relatively homogenous. 

%%%%%%%
%\subsection{Other domains that might benefit from procedural guidance}
\subsection{Deeper work: End to end scientific work}
%Social computing environments that model richer work in more domains using procedural support and roles
Scientific experimentation forms a critical component of scientific work across many domains in the natural and behavioral science. Experimentation provides one method of enquiry; other ways to test hypotheses include conducting surveys, observational studies, and more. Furthermore, \textit{scientists} perform a range of activities including: 1) data analysis, and 2) comunicating the results (e.g. by writing a paper). One key challenge in such complex work is coming up with the initial design(s) that can be refined. This dissertation demonstrates that procedural guidance enables people to transform an intuition to a structurally-sound experiment design.
%Starting with being able to understand and reason about the choices, there are many concrete activities that 

% a virtuous (??) 
One popular process for writing includes creating an outline, converting the outline to a draft, and editing the draft. Past research systems (like soylent) have supported people in editing papers. However, few tools support creating the outline and the first draft, arguably an onerous task. Research journals and conferences provide templates for formatting guidelines, but the argument and the authors need to generate the content and the argument. EteRNA participants used system-provided templates to write up their results and share with others. How might procedural systems assist?
%see evernote

%multiple sources: experienced campaigners’ minds, books/tutorials, and internet posts. 
As is common for complex work, insights to write are distributed across people, static documents, and online resources. Experienced campaigners know the success criteria, mental scaffolds to help with writing, and access to other people to invite feedback on their work. Novices lack all these. Explicating experts' tacit knowledge and showing them to novices in the context of the work can be useful. To demonstrate the variance in the scientific writing process, let’s consider two contrasting examples: the methods section and the discussion section.

The methods section follows standard guidelines and many expert writers might find this to be easy to write up. However, writing the methods section is not straightforward for novices. The method section describes the study hypotheses, choices of measures and method of enquiry, and all relevant decisions taken while running the study. Furthermore, these details need to be mentioned in standard ways so others can understand and recreate these instructions. A procedural guidance system can rely heavily on templates in this case to ensure that novices don't commit standard \textit{mistakes}. The key insight here is for the system to support people in exploting the rather specific structure of the methods section.

The discussion section is far less templated though. The discussion section needs to bring together the ideas presented in the paper, the methods, and the results. In doing so, the discussion section needs to link ideas from multiple sections of the paper: claims about the novelty of the work, ways in which this work extends current enquiry, the actual results and how they deviate from the expected results, and the new questions created by this enquiry. A procedural guidance system for writing the discussion section can use multiple techniques; it can 1) identify the research question; 2) use rubrics to prompt the writer to reflect on their claims; 3) show examples from other discussion sections to show how to write specific portions; and 4) use checklists and peer feedback to improve clarity. The key insight here would be to help people explore all the options or questions to answer in the discussion section.

Such suggestions are preliminary; this dissertation's experience strongly suggests rapid iterations using pilots to understand the success and failure of different techniques. 
%The difference between domains differences too and it becomes interesting to note how this might shake out — that is what makes this research exciting.

\subsection{More domains for citizen-led scientific investigations}
This dissertation used microbiome research as a petri dish because it’s nascent, personal, and motivating. Health related fields with devoted communites are a good match; these include nutrition, brain zapping, and more. However, the techniques develoepd in this disseration might not apply directly: different fields have widely accepted protocols of scientific research that might not be obvious to an outsider. Systems for these domains needs to provide appropriate rules and constraints that meet the objectives and processes fo these fields. E.g. tDCS folks are interested in testing whether brain zapping improves cognitive performance. The system can support people in using standard metrics and implementation. Another domain of enquiry is cultural psychology: people have theories about their (and others') behavior and research is limited [weird]. Between-subjects experimentation might not be the most appropriate method of enquiry; observational studies might be more appropriate.
%Thinking more broadly, scientific work is not the only domain of application for this work — art (crowd art) can also learn from procedural rules [jennifer jacobs procedural art work] because it shares three features with our work: xx, yy, and zz.There are limits too: 1) when people do not have relevant expertise or when teaching them takes a lot of effort — consider journalism; 2)  when people don’t stand to gain?; 3)  when different people don’t bring in different expertise

%%%
\subsection{Designing efficient procedural support}
%highlight open questions?
%Procedural support provides a remixed version of computational thinking. 

Computational problem-solving focuses on four key processes: abstraction, decomposition, generalization, and pattern matching [??]. With procedural guidance systems, the key difference is \textit{who} performs the work in these phases. For traditional computational problems, the (algorithm) designer performs these steps and implements them in the software. For a broad class of problems, a new \textit{learning} approach is popular: the designer decides the input and output specifications and provides the data; the machines' underlying structures perform the four phases independent of the designer.The trouble with computer procedures is that are interpreted literally; people are wiser. 

For complex socio-computational problems, the four phases might be implemented by different groups: the system designer might abstract the problem to a known class, provide decomposition and computational support using the system while people might perform open-ended tasks of generalization and pattern matching. This approach reconsiders procedures as instructions interpreted by machines \textit{and} people.
%  read computational thinking

\subsubsection{Integrating procedural support for your task}
%this heading is kinda useless
Based on this dissertation's struggles and results, we see the following process for how to integrate procedural guidance for a complex activity.
% (add figure)
\begin{itemize}
\item Examples, checklists, and templates provide procedural support: Procedural guidance works well when the sub-examples provide a direct link to the overall picture (e.g. hypo vs exp design).  Multimodal interactions for one or more of these can be useful too: e.g. people could see JIT videos of experts' work to draw inspiration rather than just looking at a final output. A key responsibility of the system is to handle the interaction between subparts or  provide examples of the sufficient abstraction; tackling all cases might be difficult. Dividing a complex artifact into specific subparts is difficult and requires starting from "good enough" places rather than providing perfect examples etc. The challenge is also of correctness: how do you teach complex ideas without making them wrong. Choosing appropriate examples etc is important.
\item People identify: People are good at identifying than generating [?? analogical thinking]. With procedural support, people aren’t blind users of the system but they actively translate what’s provided to them and do it back. Consequently, the support needs to be simple enough for the users to understand -- for novices, this might mean using simpler terminology. 
\item People transfer these ideas to their work/domain: To make the links concrete, the examples should be specific. To reduce the load of transfering the ideas, the support should be provided in-situ. Consequently, people will struggle when understanding procedural support itself takes effort or when the examples/checklists are too big or when this still requires expertise. 
\item Reviews from others: This should be complementary to the designer's work.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%
\section{Patterns} 
%"What are the research issues for advancing and crystallizing patterns in this area?" - srk
%     see email threads with tricia
How can computational systems reify different work environments for people to participate in personally meaningful scientific work? Answer this question requires aswering many research questions.

\subsection{Where does procedural support come from}
%Putting the knowledge together?
Many heterogenous resources---documents, experts, prior research artifacts---provide useful procedural guidance system. This dissertation employed many of these and found that each option provides different trade-offs.

\begin{itemize}
\item Static documents such as introductory books\\
Books aimed at novices provide a great resource to bootstrap procedural guidance. The systems described in this dissertation liberally used insights from multiple books including Martin’s Designing psych experiments [??] and the Blue book for social computing [??]. Using books for introductory teaching provide multiple advantages: 1) they're intended for novices, therefore the information is basic; 2) the problems sets provide a corpus of common errors. However, these resources also suffer from three challenges. First, they focus on conceptual understanding rather than providing procedural workflows. Second, they might not provide appropriate \textit{chunking} size. For instance, implementing a book's insights line-by-line might lead to a highly bloated system that might not help the user arrange relevant material in useful, big-sized chunks. 
%Third, 
%Social computing systems will need to identify ways to convert learning resources (like books) to specific useful things in systems. Think 

%provide examples for all
%create a table for this
\item Procedural corpus from online fora, blogposts, wikihow, HCI artifacts\\
People provide copious details on topics of interest. On multiple online fora, people share details of their goals, their procedure, and even their evaluation of different techniques. E.g., wikihow provides a corpus of how to perform a wide range of activities from gardening to writing a letter. Comments on popular news articles (e.g. NYtimes) have plenty of details about people's attempts. Such approaches provide more practical knowledge that might be closely related to the author\textquotesingle s content. 

Prior research attempts have successfully draw insights to help people do things. Learnersourcing has demonstrated that learners can identify patterns that can be useful for others [juho]. Other work has demonstrated that conversations can be bootstrapped from fiction [msb-empath]. Mining online forum data to identify procedural rules can reduce the designer's efforts in creating these procedures themselves. Finally, many HCI research systems for programming and witing identify key procedural insights that can bootsreap procedural systems. 
%While not procedural, prior research has demonstrated that people's beer preferences can be used to create a taxonomy of beer  [julian mcauley].
%crowdsourcing procedural rules

\item People \\
Expertise is a precious commodity; just teaching how to provide feedback takes plenty of effort from experts. For this dissertation research, microbiome experts were wary of providing feedback on the platform partly due to the time efforts needed. 

There are two ways to deal with this: First, active demonstration by experts where experts do something as part of their regular workflow and then ask people to follow. A step-by-step approach might be tailored to a person\textquotesingle s current bilities, ala zones of proximal development. Programming by demonstration makes one case for this [scott]. Second, passive tracking enables different activities performed by experts and then subsequent processing can draw rules form them. While working with people, being mindful of their time and effort is important.
%[??motif] 
\end{itemize}

Like previous sections, this dissertation strongly recommends iterating with pilots to understand the success and failure modes of these resources and how to combine them in useful ways.
%-- pamhinds --
% zones of proximal development

\subsection{Learning tools for end-users to perform higher-order scientific work}
Learning has always been lifelong. Rapid change and the ready availability of online resources make it even more so. This dissertation seeks to place learning experiences at the right time for people to use them. This offers both theoretical and practical benefits. In the learning sciences, Bloom’s taxonomy shows a hierarchy of ways of engaging knowledge, from remembering facts to evaluating theories. Traditionally, this diagram invites a discussion of classroom learning objectives. Such an order implies a potential research trajectory. How might we leverage similarities between Bloom’s taxonomy of learning and the hierarchy of social computing roles? How might we design online learning environments for people to move their engagement with knowledge up the hierarchy? 

The interaction between procedural learning and social computing raises several key research questions: how do we bake learning in systems? Do such approaches demonstrate learning gains? Consider the roles that emerged in the Gut Instinct platform. For instance, ideas from legitimate peripheral participation [??] propose that engaging people in small tasks can assist in subsequently increasing their contributions. However, this idea has been tested for work that is less conceptual and less complex than experimentation. Does people's contribution across roles increase linerarly for complex work? Do different roles demonstrate different gains?
%expound on this

%todo- missing subsection about learning gains
%which examples are better than others


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods} 
%"Reflecting on the methods you employed – system building, studies, etc. What worked best? What effort was wasted? What things would you do differently? And what would you recommend (or not) to others? (This might go adjacent challenges.)" - srk
% read MSB - diss + the EA about social computing

%%
\subsection{Building a science of social computing systems}
Social computing systems leverage ideas from multiple disciplines and require substantial engineering. A science of such systems can support early-stage researchers in the face of high-barrier to entry and multiple points of failure. This dissertation identifies questions of prototyping, co-design, and emergent behavior as important issues. We need a combination of theory, prototyping tools, and benchmarks. 

This section raises three questions: how do we design systems that are 1) quick to build, 2) that meet their objectives. Secondly, how can people engage with these systems: such that more people use them, and that these happen in collaboration with others. I also share ways to do this and provide examples.

%%%
\textbf{Prototyping}: How can we rapidly build, debug, and improve social computing systems? For instance, evaluating social computing systems is difficult: 1) there are multiple features to evaluate; so there can be multiple measures; 2) particular evaluation schemes might not be well-suited; 3) finding participants is hard.

%1 - many features
All three systems presented in this dissertation have multiple features. Traditional HCI interface research might suggest evaluating these features separately. Unfortumately, this is prohibitive: 1) more features mean more users needed and 2) more effort from the designer(s). Social computing evaluation "requires" more holistic testing than usability testing or feature testing. In many social computing is less about the system but what the system enables groups of people to do -- it is inherently a co-created process among multiple parties. 

My suggestion: One approach might be to categorically separate measures for system evaluation (e.g. do people collaboratively create better questions using Docent?) from feature evaluation (e.g. does Docent‘s edit feature help people improve another user’s question). A clear separation might help system designers sort the evaluation components in order of importance, assign different quality thresholds (e.g. controlled experiments vs observational evaluation), and communicate overall evaluation effectively to the research community.
%combine with below

%1.5 - evaluation measures
A “successful” system balances an objective function across multiple factors: user longevity, activity, engagement, deeper work done not to mention qualitative measures like quotes etc. However, despite all these activities, a designer still needs to identify specific measures. Identifying the key purpsoe of the social computing system is important. 

Focusing on just one metric is incomplete; however, claiming success based on any pick and choose of metrics (including hand-picked participant quotes -- provide all the quotes or provide none) seems intellectually dishonest too. This is the social computing version of p-hacking. I'll call this participant quote hacking. There needs to be stricter hypothesis-driven testing where people clearly lay out the intervention and the emasure and identify others are side-issues. Consistent with recent thrust in social sciences, these hypotheses must be pre-registered (which means also write the scripts etc..). This key metric idea comes with the added advantage of focusing the designer's attention on one thing.

My suggestion: Make measures about human activity, not the features. In social computing, the tech doesn't do it, but rather people do it. Example from gi: we tried testing for learning but failed and also realized that's not what people cared about -- so we threw it out and instead focused on one thing in subsequent iterations with hypothesis-driven testing. 

%2 - Pilot and evaluation process
between-subjects experiment is the gold standard to evaluate but strict manipulation of one broad condition is difficult. 
-- mechanistic explanations are hard to find
-- ask people -- foldit has already found that asking people how they do it is super useful
-- build the system in a way that you can find out where the problem is. e.g. in galileo, we learnt that people drop off at the first step, but we still don't know whether it was because 

My suggestion: ..... 

%3- participants
Every design-build-deploy cycle requires multiple iterations with groups of people - this has been the bottleneck in system development for this dissertation. How do we find people to pilot these sytems? Friends and labmates aren't good proxy -- they might be motivated to do more plus they might know about the research. Paying crowdworkers doesn't work either becuase extrinsic motivation totally screws up intrinsic motivation of doing something. But also, these people are not representative of the real set.

My suggestion: One way is to partner with orgs that care about the topic. In our case, we partnered up with American Gut and multiple communites. Asking their leaders to be early users provides multiple advantages: 1) they provide a good proxy and adequate representation of the people; 2) more experience -- that's why they're the leader; 3) gatekeeping - reduce chances of harm. But of course, this increases pressure and time commitment for people, so need to be mindful of that. 

%learning gains from the previous section

%%%
\textbf{Emergent behavior and limits}: How do interdependent, community-driven systems work in practice and how do participants self-organize? Finally, what are the limits to solving problems with social computing? 

The results are not even-keeled across users: some do more than others [pareto], many drop out [??], and take different roles.

Principles derived from psychology, organizational behavior, and previous systems provide a starting point ~\cite{fromstatement}. However, the designer still needs to understand, select, and test multiple approaches before baking them in software, which necessitates further user-centered development and evaluation. Furthermore, small sample studies can also be poor predictors of emergent behavior in new systems. 

e.g. finding participants was a challenge 
What roles emerge and how do we design for those?

%4 - users -- what they did and how
The results of an algorithm are consistent across the thing; this is not true for social computing. 
Also, why did some people do this well but others did not. learn from the medical field -- where you wanna know why an intervention worked for some poeple but not for others
My suggestion: focus on human activity. another example: focusing on human activity also led me to find roles in different things that people did -- this was useful and might have been otherwise lost. 2) Also, conduct surveys etc. and ask them, 3) sometimes it's not about the system at all. people have other motivation.

%%%
\textbf{Co-design with other experts and users}: How do experts across multiple domains contribute towards building systems that support domain-specific enquiry? How might systems support co-design by users?
Many diverse efforts, including Precision Medicine Initiative (allofus.nih.gov), Zooniverse (zooniverse.org), and Foldscope Microcosmos (foldscope.com), might benefit from using this dissertation’s principles to diversify and deepen citizen contributions. However, building such a network requires effort that are tangential to research. 

This dissertation features contributions from multiple communities—such as kombucha enthusiasts, Open Humans. The research papers feature xx co-authors from yy fields including microbiology [GI-Galileo], cognitive science, learning psychology [??2], and systems [??1,7]. Working with multiple domain experts brings great value and learning but also multiple challenges. It requires finding common ground and developing a shared vocabulary. 

My experience suggests that grounding the conversation in prototypes invites specific feedback from domain-experts that helps the system designer draw higher-level principles. Less emails and Regular meetings [Trust breaks down paper] help catch early errors.

For example, an early prototype with chat ideas floundered at the prototyping stage itself because experts mentioned that finding details in people's conversation will be expert time-intensive and they did not want to do that. 

However, building the system still need work. Might there be ways to automatically bootstrap multiple prototypes for specific atomics tasks (asking questions, adding responses) that enable these conversations to be faster and more detailed as easy as templates? Wat mgiht be a temlated way to develop social computing systems? Does it need to be digital? Could experts paper prototype different parts of the system? (papier machie system)

%%
Furthermore, many collaborative projects lead to novel opportunities and transfer of ideas in all directions. how do we make this more systematic? Gut Instinct collaborators have brought their diverse insights to human-computer interaction work; they have also taken HCI techniques home. Some of them now use needfinding and low-fidelity prototyping techniques before beginning complex software development. While these questions are difficult to answer in the abstract, creating ways for social computing researchers to share their ideas towards building such knowledge base can be super useful.    %https://en.wikipedia.org/wiki/$Co-production_(public_services)$




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%From Data to knowledge to wisdom
This dissertation research provides a vision and prototype systems for complex work by drawing on insights from interactive systems, social computing, and learning theory  for enabling people to perform personally meaningful work. By doing so, this thesis intends to democratize expertise and provide ways to meaningfully embed computation in society. 
