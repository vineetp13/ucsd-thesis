


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Collaboratively generating ideas}

\section{Dual objective functions in learning and crowdsourcing}
Combining university classes in psychology with editing
Wikipedia articles led to improvement in the scientific
content of over 800 Wikipedia articles while students
learned about the topic they edited [25]. Similarly, Kim et
al. asked learners to create how-to video segments as part
of an online curriculum [35]; the student-created videos
then became a learning resource for the next cohort. 

Some crowdsourcing offers a dual objective: user-facing
goals include fun (e.g., Peekaboom [2]), authentication
(reCAPTCHA [3]), and learning (Duolingo [28]). Under the
hood, these tasks simultaneously label images, transcribe
text, and translate phrases. Such crowd work can also bootstrap machine learning [8]. This paper is distinct from prior
work (Figure 2) in leveraging peopleâ€™s individual lived
experience, knowledge, context, and folk theories, rather
than treating people as interchangeable respondents.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Collaboratively generating hypotheses}
Source of hypothses: data seen, intuitions, folk theories, experiences

\section{Learn-Train-Ask: From Intuitions to Hypotheses}
\section{Design process: CHI 2017 Gut Instinct}
People created vague unfalsifiable hypotheses

\section{Docent: System Design}
\section{Evaluation: Scaffolds For Better Questions}
 \subsection{Hypotheses Test}
 \subsection{The effect of learning and training on questions}
 \subsection{Which topics did the questions deal with?}
 \subsection{How novel are the questions?}
 \subsection{Emergent behavior, Engagement, and Growth}

\section{Discussion and Design Implications????}
\section{Summary???}

\section{Self-tracking Offers Insights but Not Causality}
??????

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
