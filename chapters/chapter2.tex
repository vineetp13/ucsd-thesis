
\chapter{Related Work}

\begin{quote}
\emph{This chapter summarizes related work in citizen science, crowdsourcing, and  lead-user innovation that informs the design of systems in this thesis. The key contribution of this thesis is integrarting learning in social computing to enable complex work. }

\end{quote}
\vspace{0.25in}


This work does three things
1. Provides an overview of the many successes of citizen science and suggests novel opportunities
2. We then discuss lead-user innovation and how people meet their needs, and how that can be scaled to social work
3. We then show how integrating learning in crowdsourcing can help -- learnersourcing etc.. 

This chapter provides an overview of related research; chapters dedicated to specific systems discuss additional research that informs that design of those systems.

Contributions
1. This work explores how
online learning and process training systems, combined with
peer collaboration, can help people learn similar skills that
can be useful in scientific and design domains.
2. 

%Related work

\subsection{what is the current state of the art in learning + social computing}
%%building up solution space - more work
How do we integrate learning — we don’t know? This thesis explores integrating learning in social computing for complex, creative work with two goals in mind: efficacy of the resulting systems (i.e. usability, correctness, and existential evidence) and generality of the underlying techniques upon which the tools are built.

%needs some more about learning here
Task-specific guidelines are used for review and other tasks, but existing idea offer limited ways for novices to create the artifact in the first place. Procedural training and reifying genre work is a new approach that enables people to do more. 

%Learning resources proliferate and people learn from personal interest/hobby but it's not directly linked to their personal thing.
Creative, open-ended work has rich pedagogical value. Online work, like online learning, requires appropriate scaffoldings, such as rubrics~\cite{Boud1995, Kulkarni2013peer}, decision trees~\cite{Lee2016,Yu2006}, tutorials~\cite{Andersen2012}, and quick expert guidance~\cite{dow2012shepherding}. Similar to general critique of pure discovery learning~\cite{Mayer2004}, simply asking participants to “figure it out” would be poor pedagogy. Hence, Gut Instinct introduces a guided discovery learning approach as Mayer advocates: expert-curated learning materials help participants start, with discovery following. Recruiting learners as citizen scientists offers a Problem-based learning experience with context and motivation for the material students learn~\cite{Savery1995}. In principle, these real-world problems also provide a yardstick for measuring learning. 

%“Lead users are rarely experts by themselves. They are novices who find themselves at the right place, with the right tools (that they might have created themselves), and who have the courage to follow through.” Lead users have created different—and in some cases better designs— than experts. Citizens have successfully solved expert-defined problems as sensors or algorithms. For long, citizen scientists have been amateurs who contribute scientific work - typically a) under the guidance of experts, b)  on large projects, and c) that are intractable with existing scientific resources.Citizen science for long has continued work with this row-filling model of contribution rather than providing a truly independent, participatory experience. 

%%%%%%
\section{Successes and limitations of citizen science}
Science is increasingly networked, multidisciplinary, and
open~\cite{Pandey2017}. For instance, LIGO’s pathbreaking discovery of
gravitational waves brought together over 100 researchers
from over 100 institutions across 18 countries (ligo.org/about). 
Scientists increasingly share data and results faster (arxiv.org). 
Large scientific projects, like the Human Genome Project, 
took to agile science by sharing methods, data, and insights to 
collaboratively speed discoveries. Scientists also form global 
collaborations to accelerate research in nascent scientific domains, 
like the Earth Microbiome project (earthmicrobiome.org).
At its best, institutional science has benefitted immensely
from large-scale global collaboration. Complementing this,
many online projects enable people to help scientists: annotating scientific papers~\cite{Good2013}; labeling galaxies~\cite{JordanRaddick2013}; folding protein structures ~\cite{Cooper2010} and providing microbiome samples~\cite{McDonald2018}, CPU cycles (worldcommunitygrid.org), or personal data (openhumans.org). 

Often, when citizens participate in science, it is as “embedded sensors” that 
are aggregated by experts. Public involvement in scientific endeavors continues
to be largely limited to performing tasks just beyond the reach of computers.
A classic example is Audubon’s Christmas bird count, run since 1900
~\cite{Audubon2016}. Online examples include reporting flower blooms in
Project Budburst~\cite{BoulderColorado2016}; recording wildlife activity~\cite{Faridani2009a};
identifying galaxies from satellite imagery in GalaxyZoo~\cite{Zooniverse2007}; and biochemistry games: finding protein structures in
Foldit~\cite{Cooper2010}, synthesizing RNA molecules in EteRNA~\cite{Lee2014}, and aligning 
nucleotide sequences in Phylo~\cite{Kawrykow2012}. At their
best, these citizen science platforms yield novel insights.
For example, Foldit players discovered protein structures
that helped scientists understand how the AIDS virus reproduces~\cite{Coren2011}. 

 Such bite-sized contributions is not without reason—a lot of
scientific work requires deep conceptual knowledge and 
training in scientific process to perform useful work. Most
citizens lack the time, resources, and motivation to develop
narrow, unique skillsets. Expanding the depth and breadth of work 
performed by citizen science communities would be useful.

\subsection{Internet-scale science misses people’s lived experience}
In the quest to get people to track, measure, accumulate, or
sort both digital and analog data, citizen science has overlooked the massive opportunity of leveraging people’s
unique advantages: our skills as reflective, creative thinkers
who generate theories about the world, including ourselves.
People can offer more than just their data and perceptual
skills: they create theories, right or wrong, about a wide
range of topics including emotions~\cite{Johnson-Laird1992a}, motivation~\cite{Markus1991}, or
diet. These may be observational theories~\cite{Kempton1986}, folk theories
passed in a family/culture across generations~\cite{Gelman2011}, or ideas
brainstormed in online communities~\cite{23andme2016}. Perhaps, these intuitions can provide a starting point for personally meaningful scientific work that also assists the scientific community.

\subsubsection{Can people be scientists rather than just sensors?}
Advances in precision medicine have demonstrated the need
to engage people in uncovering and sharing insights~\cite{Aronson2015}. People
are highly motivated to improve their health outcomes,
more so if they suffer from a condition that severely affects
their quality of life, naturally forming communities. For example,
patients from the Amyotrophic Lateral Sclerosis
(ALS) community on Patients Like Me (patientslikeme.com)
organized a study to track effects of Lithium on their symptoms
~\cite{Wicks2011}. This is not surprising; lead users excel at tackling
need-intensive problems where they can use their lived
experiences to identify problems, try solutions, and readily
observe the effects~\cite{VonHippel2005}. Other organized communities like
Quantified Self hope to uncover lifestyle patterns that may
improve their productivity and health outcomes. The word
‘self’ belies the fact that such movements are highly collaborative:
amateurs frequently share experiences and invite
feedback on online fora (patientslikeme.com) and blogs
(ibsgroup.org). Millions follow these ideas and some incorporate
these intuitions in their lives. How can people expand
their insights into scientific work?

Most scientists develop their skills through an apprenticeship-
based graduate school experience. Apprenticeships emphasize
hands-on experience with individualized, taskspecific
feedback~\cite{schon1984reflective}. Scientists possess a wealth of declarative
knowledge about their domains (e.g., how to set up a
randomized controlled trial), and also procedural knowledge
—some narrow, some broad —towards getting things done
(e.g., improving fMRI signal intensity by having participants
consume cocoa beforehand~\cite{Francis2006}). This work explores how
online learning and process training systems, combined with
peer collaboration, can help people learn similar skills that
can be useful in scientific and design domains.

%todo-fix cites
Modern science is increasingly collaborative~\cite{Nielsen2012}: citizens count bird species, identify galaxies, edit protein structures, and create novel hypotheses ~\cite{Cooper2012, Pandey2018,Zooniverse2007}. One reason is that different people provide different expertise that can vet claims and fix mistakes~\cite{kane2009s}. Collaboration benefits creativity when it brings different perspectives that build on each other; it impedes creativity (or worse, causes regression) when—through groupthink—it spreads biases rather than removing them~\cite{starbird2014rumors}. A humbling example of the power of fresh eyes: volunteer citizen scientists identified a new class of galaxies (“green pea” galaxies) after researching green blots on Galaxy zoo images; experts had dismissed these images as apparatus error~\cite{cardamone2009galaxy}. Such collaboration requires strategic isolation: providing just enough scaffolding to keep biases independent, while not stifling original ideas for bottom-up knowledge creation.
While public contributions have supported institutional science; it’s rare for citizens to design
their own experiments. Despite a predetermined goal and a formalized process, experimentation
requires making situationally-appropriate decisions. A dependent variable may produce crisp
numbers but feedback on the experiment design itself is more multifarious. Good experiment
design is inherently user centered: how will participants interpret the instructions? Experiment
designers need awareness of others’ interpretation of their ideas and asks. Feedback and iteration
might be key to creative success, especially for novices. Feedback can be provided by experts~\cite{dow2012shepherding, schon1984reflective}, peers~\cite{Boud1995, Kulkarni2015b}, software~\cite{Dantoni2015, Head2017}, or even oneself~\cite{Boud1995,schon1984reflective}. While feedback from novices can
potentially improve both structure and content, it can also emphasize superficial issues over the
underlying structure~\cite{chi1981expertise}.

%%%%%%%%%%%
\subsection{Microbiome research: a petri dish for making scientists}
Understanding the human microbiome requires insights
into people’s lifestyles. Microbiome science is nascent, highly contextual, and personally motivating.
The human gut microbiome is the community of microbes
(and their gene products) interacting in the human gut.
However, research has only scratched the surface of understanding the microbiome and using it to improve our wellbeing. The American Gut Project (AGP) is the world's
largest crowdfunded citizen science project~\cite{KnightLab2016a}. AGP
participants contribute their samples for bacterial marker
gene sequencing and analysis~\cite{Debelius2016}. Participants then receive
a summary of their results with all their raw data. Anonymized data is publically available. AGP seeks to build a
comprehensive map of the human microbiome, and identify
its healthy and unhealthy components.
People hold the key to understanding the gut microbiome
The structure of the human microbiome is influenced by
many factors, including age, genetics, diet, and xenobiotic
and antibiotic use~\cite{Gill2006}. The gut microbiome in particular
plays an important role in metabolism and immune system
development, and some microbiome dysbioses have been
associated with diseases such as obesity, inflammatory
bowel disease, type I and type II diabetes, autism, multiple
sclerosis, and malnutrition~\cite{Cho2012}. The human microbiome is
impossible to understand without information about its host~\cite{Debelius2016} and many influence factors remain unknown. Teaching
people about the gut microbiome and having them guess
associations between the microbiome and health and disease states can potentially accelerate the process of discovering links between diet, disease, and lifestyle factors and
the gut microbiome.

The human microbiome is the collection of all microbes and
their genetic components in and on our bodies. It is highly
personal: each of us hosts a different collection of microbes,
and this collection is influenced by our environment, diet,
health, lifestyle, and genetics. A major scientific effort is to
better characterize and understand this diversity and the
causal factors for it (hmpdacc.org). This requires engaging
diverse participants at scale.

The American Gut Project (americangut.org) offers a
crowdsourced opportunity for people to get a microbiome
sampling kit. To date, more than 13,000 people have participated.
Participants submit both a physical sample and fill out
a survey. Analysis has revealed lifestyle-microbiome correlations
of dog ownership and beer or vegetable consumption,
among others. Currently, the survey questions are handpicked
by a small group of scientists. Can opening up the
question-asking process to the world yield additional insights?
How can people’s situated knowledge supplement institutional
science? Herein lies the opportunity for \textit{Docent}.


So, we find that there are three main challenges for citizen science: 1) make it personally meaningful, 2) deepen the contributions that citizens can make.
3. 1. Improve the quality of poor contributions — and improve the quality of max contributions 
    1. — hockey stick curve 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lead Users Succeed When People Know What to Do and How to Do It}
-- Techniques and systems to build expertise in people (toolkit?)

1. GitHub: enables storing code, sharing code, great, but doesn’t teach how to program
2. Technology systems currently use existing capacity and takes away the context that they have — how do you build capacity — people have the context 
3. techniques to build expertise in people
4. difference between lead users and end users? 
5. examples: UN example (see eric vh)

Contribution: People haven't thought about lead/end users as communities. This thesis does.
\subsection{Lead users hack and track}
Lead users~\cite{VonHippel2005} collaborate online to build software
(github.com), create novel hardware \& reference designs
(openaps.org), and share personal data (quantifiedself.com,
openhumans.org). Some go further still, e.g., the transcranial
direct-current stimulation community draws ideas from scientific
papers to attempt self-experiments (reddit.com/r/
tDCS). In a few exceptional cases, lead users have even authored
scientific papers, e.g., Open Artificial Pancreas creator
Dana Lewis discussed the benefits and challenges of firstgeneration
automated insulin delivery at the 2016 American
Diabetes Conference~\cite{DanaLewis}.

Why do people do this? Curiosity, personal learning, and social
comparison are three reasons~\cite{Reinecke2015}. A massive interest in
personal genomics (over 1 million 23andme participants)
and, more recently, the human microbiome (13,000 American
Gut Project participants, americangut.org) demonstrate
people’s urge to understand what makes them who they are.
Users of these platforms send data, answer survey questions,
and discuss on fora. Some even use online lectures to understand
concepts of genes, phenotypes, and microbiota they
may not have perused otherwise~\cite{23andMe2017, Knight2016}.

However, community-driven approaches to understand personal
health and well-being largely reside outside the realm
of institutional science and medicine. While some fads and
beliefs are questionable at best, on occasion these communities
break new ground that may provide widespread value,
such as fecal transplants to alleviate Clostridium difficile infection
symptoms~\cite{Brandt2012}. Some doctors recommend that patients
track their symptoms and reflect upon them to find
insights. Putting people in charge can help them find significant
relief for ailments like chronic migraine~\cite{Gawande2017} and provide
researchers and clinicians with useful patient data
(smartpatients.com). Insights from N = 1 studies have helped
crack scientific puzzles about the working of the mind~\cite{V.S.Ramachandran1998},
heart, and microbes~\cite{Weisse2012}. More broadly, people have
followed their personal intuitions to design and build
products that meet their needs.

When are such personal experiences worth paying attention
to? For every intuition proven right, many more may be
closer to snake oil — e.g., the widespread belief in the utility
of probiotics despite limited evidence~\cite{Bonifait2009}. The global internet
increases the proliferation of both powerful and questionable
ideas: sharing speculation is fast while evaluation remains
slow. Moreover, people develop intuitions of cause and effect
that may or may not be correct. Current online forum
designs prioritize discussion — sharing personal details in
long, free-flowing text — over structure, succinctness, learning,
and potential scientific utility~\cite{Thomas2002}. What kinds of scaffolds
and structure may help people generate better ideas and
enable researchers to identify promising insights, without
sifting through freeform text?

\subsection{Community Support for Lead-user work. Example: Health}
Self-tracking Offers Insights but Not Causality
People have questions about their health, but lack the expertise and resources to scientifically
investigate them. These concerns are especially acute when multiple factors interact. Despite
knowledge gained from lived experiences, people lack the procedural tools to gain the causal
knowledge they seek. Many self-tracking efforts suffer from structural flaws that prohibit people
from actually learning what they’d like to know~\cite{Choe2014, Li2010a}. A frequent error is mistaking correlation
for causation~\cite{Munroe2009}. People falsely believe that when one event follows another, the initial event is
the cause: post-hoc ergo propter hoc. At the same time, professional science suffers from structural
biases. By creating controlled experiments (as opposed to tracking oneself), people can test their
intuitions at a larger scale, potentially unearthing novel results. How can we train people in designing
and running experiments to answer their personally-meaningful questions?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Complex, Creative work using Social Computing}

Complex work using social computing requires two things:
1) lowers the threshold for complex work, and 
2) inject learning to accomplish these smaller tasks

This thesis demonstrates how to inject learning in social computing systems
-- rather than divide into many small pieces (microtasks), build capacity among people. 

\subsection{Crowdsourcing Architectures for Complex Work}
Canonical crowdsourcing breaks larger tasks into microtasks; algorithms specify the division,
dependency, and agglomeration activities while workers perform small tasks supported by task-specific
guidelines~\cite{kittur2012future}. Crowdsourcing has worked around the challenge of limited procedural
learning by providing some learning in the interface, or by leveraging experts (improve).

Systems like Foldit and EteRNA powerfully show how carefully-constructed interfaces provide
novices with the task-specific expertise to solve problems that only experts previously could~\cite{Cooper2010, Lasecki2012, Lee2014, Zooniverse2007}
Foldit introduces an interface and 3D game for specifying low-energy protein structures
to a direct manipulation game. For tasks that don’t have as a crisp visual analogue as protein
folding, people need better conceptual support. For more abstract tasks, CrowdLayout and Cicero
provide guidelines and static rules that crowdworkers can use to reason about their choices and
to improve layouts of biological networks~\cite{Singh:2018:CCD:3173574.3173806, chen2019cicero}.
Leveraging existing expertise is another approach for complex knowledge work. One strategy
directly employs experts’ just-in-time feedback to improve crowd work~\cite{dow2012shepherding}. Workflows manage
experts for open-ended work like developing
interactive prototypes~\cite{Retelny2014}. Flash Organizations
uses automated hiring, a hierarchy with
a central leader, and optional team leaders for
collaborative projects like product design~\cite{Valentine2017}.
Another strategy creates roles that enable
more experienced crowd members to orchestrate
the work. Ensemble supports leaders in
guiding and constraining crowd contributions~\cite{Kim2014e}. Role-based approaches confer three benefits:
1) clean delineation of responsibilities improves
chances of task completion, 2) clustering
similar tasks reduces overhead and increases
consistency; 3) people can decide their
contribution levels. However, experts are expensive,
in short supply, and sometimes prone
to groupthink. How might groups of novices
perform complex work like experimentation?

\subsection{Supporting novice inquiry -- contrast this with the previous section}
Lived experience, a tight feedback loop, and strong personal motivation can yield different and sometimes better ideas than experts~\cite{VonHippel2005}. Prior work has explored collaborative hypothesis generation and testing on pre-existing data sets~\cite{luther2009pathfinder,willett2011commentspace}. Galileo offers a complementary contribution: enabling citizens to generate data on topics of personal interest.

One way to make complex tasks manageable is to divide them into distinct phases. Touchstone demonstrates the power of a semi-automated workflow integrating experiment design, testing, and analysis~\cite{Mackay2007}. Crowdsourcing has similarly innovated by creating distinct phases: break larger tasks into microtasks; algorithms specify the division, dependency, and agglomeration activities while workers perform small tasks supported by task-specific guidelines~\cite{lasecki2012real}. From these systems, our work draws the idea of dividing experimentation into multiple tasks—some self-sourced, others crowd-sourced; and introduce just-in-time domain expertise to perform these tasks. 

Carefully-constructed interfaces can aid novices with task-specific expertise to solve problems that only experts previously could. Foldit introduced 3D game for specifying low-energy protein structures via direct manipulation~\cite{Cooper2010}. Making a challenge visually salient is an effective way to on-board novices. This paper explores scaffolds that are more structural than visual.

Providing just-in-time supports, step-by-step instruction, and showing helpful supportive information are core ideas in instructional design~\cite{Kirschner2008}. Crowdsourcing systems leverage interactive guidance for specific tasks. For exam-ple, CrowdLayout and Cicero provide guidelines and static rules that workers use these to reason about their choices and improve network layouts~\cite{chen2019cicero, Singh:2018:CCD:3173574.3173806}. Others like CrowdSCIM and Crowdclass scaffold pre-task interventions~\cite{Lee2016,wang2018exploring}. Galileo introduces support during the task itself for those with little-to-no mental model of the knowledge domain. Like the Shepherd review-writing system~\cite{dow2012shepherding}, Galileo provides just-in-time support. There are two key differences: 1) Galileo scaffolds the entire creation process, not just the post-draft feedback stage, and 2) Galileo does not draw on expert time – the knowledge is implemented in the software itself. 

\subsection{personally meaningful learning and doing}
-- JIT learning techniques embedded in people’s doing


\section{Related Work - from galileo paper}
\subsection{Citizen Scientists: From Collectors to Experimenters}
Citizen science efforts span counting bird species, identify-ing galaxies, editing protein structures, and creating novel hypotheses [15,56,69]. One reason for citizen science’s success is that different people provide different expertise that can vet claims and fix mistakes [35]. A humbling example of the power of fresh eyes: volunteer citizen scientists identified an entirely new class of galaxies (“green pea” galaxies) from Galaxy zoo images; experts had dismissed these images as apparatus error [7]. This volun-teer-led discovery demonstrates the need for fostering independent perspectives while simultaneously cultivating sufficient knowledge for meaningful domain contributions. 

Efforts to expand participation in scientific research are bearing fruit: Lab in the Wild recruits anyone with an internet connection for behavioral studies [58]; All of Us aims to recruit one million Americans from all strata of society (allofus.nih.gov). Distributed data contributions from people around the world—browsing online [17], using activity trackers, and joining scientific projects—have enabled valuable insights on topics including obesity [2], aesthetic preferences [59], sleep [25], and the human microbiome [52]. Our work draws on the idea of people using their complementary insights and cognitive surplus towards expert-led scientific work [5].

A number of health and behavioral research projects enlist citizens as helpers (e.g., HabitLab [43]). It remains rare for citizens to design experiments. CivilServant enables online communities’ moderators to test policy ideas; moderators share these ideas with researchers who transform them to study designs [51]. Through the PatientsLikeMe website (patientslikeme.com), citizens and scientists created a study investigating whether consuming lithium alleviated ALS symptoms [64]. While an initial scientific study had provided positive benefits, both this citizen science study and a subsequent university study did not find benefits. Closest to our research, Tummy Trials asked participants to generate health questions, introducing a protocol for self-experimentation combining ideation and self-tracking [36].

This paper provides a general workflow for anyone to transform their intuition to an experimental design; our work focuses on controlled experiments as opposed to self-tracking or informal iteration.

\subsection{Supporting novice inquiry}
Lived experience, a tight feedback loop, and strong personal motivation can yield different and sometimes better ideas than experts [33]. Prior work has explored collaborative hypothesis generation and testing on pre-existing data sets [48,67]. Galileo offers a complementary contribution: enabling citizens to generate data on topics of personal interest.

One way to make complex tasks manageable is to divide them into distinct phases. Touchstone demonstrates the power of a semi-automated workflow integrating experi-ment design, testing, and analysis [49]. Crowdsourcing has similarly innovated by creating distinct phases: break larger tasks into microtasks; algorithms specify the division, dependency, and agglomeration activities while workers perform small tasks supported by task-specific guidelines [45]. From these systems, our work draws the idea of dividing experimentation into multiple tasks—some self-sourced, others crowd-sourced; and introduce just-in-time domain expertise to perform these tasks. 

Carefully-constructed interfaces can aid novices with task-specific expertise to solve problems that only experts previously could. Foldit introduced 3D game for specifying low-energy protein structures via direct manipulation [15]. Making a challenge visually salient is an effective way to on-board novices. This paper explores scaffolds that are more structural than visual.

Providing just-in-time supports, step-by-step instruction, and showing helpful supportive information are core ideas in instructional design [39]. Crowdsourcing systems leverage interactive guidance for specific tasks. For exam-ple, CrowdLayout and Cicero provide guidelines and static rules that workers use these to reason about their choices and improve network layouts [11,62]. Others like CrowdSCIM and Crowdclass scaffold pre-task interventions [46,63]. Galileo introduces support during the task itself for those with little-to-no mental model of the knowledge domain. Like the Shepherd review-writing system~\cite{dow2012shepherding}, Galileo provides just-in-time support. There are two key differences: 1) Galileo scaffolds the entire creation process, not just the post-draft feedback stage, and 2) Galileo does not draw on expert time – the knowledge is implemented in the software itself. 

%link to beyond being there -- rather than recreating what experts do, how about enabling novices to do differently

%%%%%%%

